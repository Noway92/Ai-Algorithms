{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM A : Genetic Algorithm for Optimizing a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=2, n_features=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.82121188 10.21216714 -0.38433606  4.00260026]\n",
      " [-4.63242991 -2.54762766 -5.25716199  2.06229474]\n",
      " [-6.81340236  8.97933212 -0.50527416  3.96732212]\n",
      " [-7.01267861 10.40881835  0.98423288  1.44791827]\n",
      " [-6.21444882 -4.8361909  -5.97171749  1.69435606]\n",
      " [-8.25258881  9.50006257  0.4439847   2.09083744]\n",
      " [-7.96038344  9.98938698 -0.05863371  3.90474132]\n",
      " [-5.771248    9.47188649  1.12319695  3.89473952]\n",
      " [-7.08297784 11.8396552  -1.16888598  2.58366117]\n",
      " [-5.65864699 -3.75111365 -4.0123749   2.22576668]\n",
      " [-4.88888806 -3.27256028 -5.46577944  1.01743171]\n",
      " [-6.22483156 -2.35052697 -6.06131461  2.6277305 ]\n",
      " [-3.61031175 -4.00232459 -5.84152666  0.33156265]\n",
      " [-7.44778686 -4.65167318 -3.93373169 -0.07820291]\n",
      " [-7.82560354  8.31456775  1.37257649  4.68013414]\n",
      " [-5.46121888 -3.14899929 -4.75767471  1.30756273]\n",
      " [-7.36486772 -2.04046811 -4.80729667  2.25615325]\n",
      " [-5.20463706 -3.21284524 -5.66316345  0.5541686 ]\n",
      " [-8.77142474  9.88452991  0.14861601  0.59045342]\n",
      " [-6.36072122  9.08025949  0.25941723  2.53914782]\n",
      " [-5.36407564 -3.79623324 -5.11398242  2.33691809]\n",
      " [-7.23802606  9.86850919 -0.79978064  2.05295013]\n",
      " [-5.61252645 -1.85904247 -3.79363751  1.16812634]\n",
      " [-5.88888331 -4.61505063 -6.70294977  1.37965581]\n",
      " [-4.75080945 -4.53573791 -4.98597648 -0.23746568]\n",
      " [-6.20004156 -4.98878487 -4.49247649  1.78842224]\n",
      " [-4.89733278 -3.19621487 -4.49484427  2.2194009 ]\n",
      " [-8.7960312  10.83257393  0.01733983  4.2761543 ]\n",
      " [-2.47190922 -3.00381274 -4.64107689  1.06268225]\n",
      " [-7.18294308 10.29236001 -1.51167689  2.97644876]\n",
      " [-8.31980158  8.76898437  0.1892383   5.79756189]\n",
      " [-5.75551764 -4.26380658 -4.4324513   2.18837727]\n",
      " [-4.80456089 -3.76505312 -5.23957952  0.71129642]\n",
      " [-6.13407864 -4.88635514 -4.44975635  2.93183035]\n",
      " [-5.48690731 -2.75984094 -3.97473795  3.03083052]\n",
      " [-9.16945232  9.92284138  1.15711357  4.17961469]\n",
      " [-6.68881621 -2.70024454 -6.80135483  1.28026832]\n",
      " [-7.07125795  7.42619129  0.87473509  3.92861056]\n",
      " [-6.85061074 -4.43081516 -6.49874516  1.49939103]\n",
      " [-6.27832637 10.23319715  1.55151927  4.59165284]\n",
      " [-8.02875413  9.37639579  0.23580499  4.02056144]\n",
      " [-6.13246109 -4.41283619 -4.2161257   1.31634722]\n",
      " [-7.58646048 11.22673772 -0.52097038  3.22559891]\n",
      " [-9.30155736  9.38128116 -0.63635151  3.62259206]\n",
      " [-7.93649691 10.74127879 -1.88880762  1.60495889]\n",
      " [-6.1411513  -5.13307021 -6.00504736  2.61811919]\n",
      " [-5.29111283 -4.1010266  -8.16175257  2.51091924]\n",
      " [-8.2998009  10.1554788   1.44926071  2.09360397]\n",
      " [-7.3466595   9.0042367   0.56529851  1.83791014]\n",
      " [-5.80461808 -2.77809905 -5.78006028  0.81269409]\n",
      " [-5.72260457 -2.47980432 -4.84218666  2.02826866]\n",
      " [-8.02508699 10.3718536   2.96844164  2.64179172]\n",
      " [-5.80518999 -3.04765106 -4.64084909  2.54242882]\n",
      " [-6.22521458  9.39918747  0.15231776  2.80762594]\n",
      " [-3.62673216 -4.03398897 -5.10352095  2.85244268]\n",
      " [-7.61606158 10.7621853  -0.26847913  4.98332294]\n",
      " [-5.05422298 -5.21677873 -5.85349155  2.03475448]\n",
      " [-6.20567097 -3.74156394 -5.5465406   2.06615754]\n",
      " [-7.8888771   9.36991706 -2.76881815  1.79095062]\n",
      " [-4.36099151 -4.687193   -5.49312559  1.99263706]\n",
      " [-4.95678817 -5.17469453 -6.53054217  1.62945381]\n",
      " [-8.28998329 10.50307894  0.20637301  3.24700642]\n",
      " [-5.51290138 -4.91977194 -6.07297202  1.51462237]\n",
      " [-6.60280524 -3.91361722 -5.74693269  1.75876727]\n",
      " [-7.80497972 10.04305597 -0.63115007  1.96522268]\n",
      " [-7.89613247  9.68139356  0.05519324  3.49814519]\n",
      " [-7.49997536 11.04518964  1.61258217  3.55312292]\n",
      " [-6.66577971  9.61102986  0.4448442   1.71199583]\n",
      " [-8.87865433 10.07663427 -1.43438237  4.45375134]\n",
      " [-5.16043965 12.2394703   0.12479347  3.60956936]\n",
      " [-6.15692786 10.28089917  1.67003601  2.59494846]\n",
      " [-6.80073712 10.05013157 -0.22951539  3.47051495]\n",
      " [-9.01107038 10.11688257  0.36853479  4.35791944]\n",
      " [-8.62421133  9.38402283 -0.18404638  2.86594438]\n",
      " [-9.31549336 11.34533536  1.29843022  1.98474505]\n",
      " [-6.0244011  -5.38266483 -5.18362011  3.26156725]\n",
      " [-4.49481111 -3.80774312 -3.94233814  3.44484038]\n",
      " [-5.06082423 -3.88519545 -4.10113359  1.16343281]\n",
      " [-8.22473981  9.47739339  0.11585513  2.41979592]\n",
      " [-6.15031452 10.1638     -0.87826324  2.87886913]\n",
      " [-3.76600835 -4.38887134 -5.33789303  0.6019338 ]\n",
      " [-6.91390964 -3.93530771 -4.62995299  2.86410469]\n",
      " [-7.74456801  9.52693394 -0.41049316  3.82561673]\n",
      " [-5.5329553  -3.12526713 -4.93395764  2.75192719]\n",
      " [-7.6360796  10.40069596  1.05593519  4.07514757]\n",
      " [-8.90501874 10.56614876 -0.51018273  3.86673163]\n",
      " [-8.27973312  9.74655076  0.07039176  3.50399658]\n",
      " [-6.35941477  8.87170727  0.10113931  6.19885537]\n",
      " [-7.27550041 -1.76515513 -6.19804125  2.13949065]\n",
      " [-4.81383164 -3.95097556 -6.44623913  2.79005395]\n",
      " [-3.56774839 -4.70123405 -5.47448776  1.96480053]\n",
      " [-6.8274948  10.09770415  0.95043011  2.23781311]\n",
      " [-3.86932954 -4.06651611 -4.78880948  2.27650788]\n",
      " [-3.91527646 -3.96113065 -5.03783091  1.36229067]\n",
      " [-6.27110405 10.20907081 -1.95086077  3.02407687]\n",
      " [-6.35215293 -3.89437013 -5.20957356  0.38142862]\n",
      " [-8.02311728  8.66231112 -0.96546823  1.81422085]\n",
      " [-5.49601931 -3.01198377 -4.78360707  2.37449432]\n",
      " [-9.05253235  9.12318953  0.46948419  4.77505415]\n",
      " [-4.87164693 -4.36588881 -5.30781387  1.08472675]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1\n",
      " 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 : Neural Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m80\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_43 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">369</span> (1.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m369\u001b[0m (1.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">369</span> (1.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m369\u001b[0m (1.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define the model architecture\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(16, input_shape=(4,), activation='sigmoid'),  # Input layer + First hidden layer with sigmoid\n",
    "        Dense(16, activation='sigmoid'),                    # Second hidden layer with sigmoid\n",
    "        Dense(1, activation='sigmoid')                      # Output layer with sigmoid\n",
    "    ])\n",
    "    # We use the MSE as asked after\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "    # Display the model architecture\n",
    "    return model\n",
    "\n",
    "model_test = create_model()\n",
    "model_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 : GA Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our class Genetic algorithm that will contains all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticAlgorithm:\n",
    "    \n",
    "    def __init__(self, model, X_train, y_train):\n",
    "        self.model = model\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        \"\"\"\n",
    "        sigmoid : f(x) = 1 / (1 + exp(-x)).\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # We created this functions because the one from Tensor Flow was taking too_long\n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Predict the outpus using sigmoid\n",
    "\n",
    "        INPUT:\n",
    "            X: The Input for the neurol network\n",
    "\n",
    "        OUTPUT\n",
    "            The prediction (numpy array)\n",
    "        \"\"\"\n",
    "        # Récupérer les poids et biais des couches du modèle\n",
    "        weights_1, biases_1 = self.model.layers[0].get_weights()  # Première couche\n",
    "        weights_2, biases_2 = self.model.layers[1].get_weights()  # Deuxième couche\n",
    "        weights_3, biases_3 = self.model.layers[2].get_weights()  # Troisième couche (sortie)\n",
    "\n",
    "        # Propagation avant\n",
    "        # Étape 1 : Entrée -> Première couche\n",
    "        layer_1 = self.sigmoid(np.dot(X, weights_1) + biases_1)\n",
    "\n",
    "        # Étape 2 : Première couche -> Deuxième couche\n",
    "        layer_2 = self.sigmoid(np.dot(layer_1, weights_2) + biases_2)\n",
    "\n",
    "        # Étape 3 : Deuxième couche -> Sortie\n",
    "        output = self.sigmoid(np.dot(layer_2, weights_3) + biases_3)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def update_model(self,individual):\n",
    "        \"\"\"\n",
    "        Sets the model weights based on the individual's parameters\n",
    "        \"\"\"\n",
    "        weights = self.model.get_weights()\n",
    "        weight_idx = 0\n",
    "        new_weights = []\n",
    "        \n",
    "        for w in weights:\n",
    "            weight_size = np.prod(w.shape)\n",
    "            poids_inter = np.array(individual[weight_idx:weight_idx + weight_size])\n",
    "            new_weights.append(poids_inter.reshape(w.shape))\n",
    "            weight_idx += weight_size\n",
    "        \n",
    "        self.model.set_weights(new_weights)\n",
    "    \n",
    "    def initialize_population(self, population_size, num_parameters):\n",
    "        \"\"\"\n",
    "        Initializes a population of individuals.\n",
    "        Each individual is an array of randomly initialized parameters.\n",
    "        \"\"\"\n",
    "        population = []\n",
    "        for _ in range(population_size):\n",
    "            individual = np.random.randn(num_parameters)  # Normal distribution\n",
    "            population.append(individual)\n",
    "        return population\n",
    "\n",
    "    def fitness(self, individual,X,y):\n",
    "        \"\"\"\n",
    "        evaluates fitness using MSE.\n",
    "        \"\"\"\n",
    "        self.update_model(individual)\n",
    "        \n",
    "        #\n",
    "        y_pred = self.predict(X)\n",
    "        loss = np.mean((y - y_pred.squeeze()) ** 2)\n",
    "\n",
    "        '''\n",
    "        Taking too long \n",
    "\n",
    "        self.update_model(individual)\n",
    "        # Evaluate fitness based on MSE\n",
    "        loss, _ = self.model.evaluate(self.X_train, self.y_train, verbose=0)\n",
    "        '''\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def Tournament(self, T, k, T_fitness):\n",
    "        \"\"\"\n",
    "        INPUT\n",
    "            T = a list of individuals randomly selected from a population.\n",
    "            Fitness = list of the fitness of all indivuals\n",
    "            k = the tournament size. In other words, the number of elements in T.\n",
    "        OUTPUT\n",
    "            the fittest individual.\n",
    "        \"\"\"\n",
    "        best = T[0]\n",
    "        best_fitness = T_fitness[0]\n",
    "        for i in range(1, k):\n",
    "            if T_fitness[i] < best_fitness:\n",
    "                best = T[i]\n",
    "                best_fitness = T_fitness[i]\n",
    "        return best\n",
    "\n",
    "    def Tournament_Selection(self, P, k, n,fitness):\n",
    "        \"\"\"\n",
    "        INPUT\n",
    "            P = The population\n",
    "            k = the tournament size, such that 1 ≤ k ≤ the number of individuals in P.\n",
    "            n = the total number of individuals we wish to select.\n",
    "        OUTPUT\n",
    "            a list with tournament winners\n",
    "        \"\"\"\n",
    "        B = []\n",
    "        for i in range(n):\n",
    "            indices = random.sample(range(len(P)), k)\n",
    "            T = [P[idx] for idx in indices]\n",
    "            T_fitness = [fitness[idx] for idx in indices]\n",
    "            B.append(self.Tournament(T, k, T_fitness))\n",
    "        return B\n",
    "\n",
    "    def Two_Points_crossover(self, individu1, individu2, n):\n",
    "        \"\"\"\n",
    "        INPUT \n",
    "            n = size of individuals\n",
    "            individu1 = first individual\n",
    "            individu2 = second individual\n",
    "        OUTPUT \n",
    "            two new individuals\n",
    "        \"\"\"\n",
    "        point1, point2 = sorted(np.random.randint(1, n - 1, 2))\n",
    "\n",
    "        new_individual1 = np.concatenate((individu1[:point1], individu2[point1:point2], individu1[point2:]))\n",
    "        new_individual2 = np.concatenate((individu2[:point1], individu1[point1:point2], individu2[point2:]))\n",
    "        \n",
    "        return new_individual1, new_individual2\n",
    "    \n",
    "    def apply_crossover(self,individuals,n):\n",
    "        \"\"\"\n",
    "        INPUT \n",
    "            indivuals = all our indivual\n",
    "            n = size of individuals\n",
    "        OUTPUT \n",
    "           list with the new_individuals\n",
    "        \"\"\"\n",
    "        new_individual=[]\n",
    "        for i in range(0,n-1,2):\n",
    "            new_individual1, new_individual2 = self.Two_Points_crossover(individuals[i],individuals[i+1],n)\n",
    "            new_individual.append(new_individual1)\n",
    "            new_individual.append(new_individual2)\n",
    "        return new_individual\n",
    "\n",
    "    def swap_mutation(self, individu, mutation_rate=0.1):\n",
    "        \"\"\"\n",
    "        INPUT \n",
    "            individu = individual we want to analyse\n",
    "            mutation_rate = rate if we want to aply our mutation on the individual\n",
    "        OUTPUT\n",
    "            a new individual using swap mutation\n",
    "        \"\"\"\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            index1, index2 = np.random.choice(len(individu), 2, replace=False)\n",
    "            individu[index1], individu[index2] = individu[index2], individu[index1]\n",
    "        return individu\n",
    "    \n",
    "    def apply_mutation(self,individus,n,mutation_rate=0.1):\n",
    "        \"\"\"\n",
    "        INPUT \n",
    "            individus = list of individual we want to analyse\n",
    "            n = size of individuals\n",
    "            mutation_rate = rate if we want to aply our mutation on the individual\n",
    "        OUTPUT\n",
    "            a new individual using swap mutation\n",
    "        \"\"\"\n",
    "        new_individu=[]\n",
    "        for i in range(n):\n",
    "            new_individu.append(self.swap_mutation(individus[i],mutation_rate))\n",
    "        return new_individu\n",
    "    \n",
    "    def sort_population(self,population, fitness_scores):\n",
    "\n",
    "        \"\"\"\n",
    "        Other way\n",
    "\n",
    "        sorted_population_with_fitness = sorted(zip(fitness_scores, population), key=lambda pair: pair[0])\n",
    "        # sort the population by their fitness\n",
    "        sorted_population = [indiv for _, indiv in sorted_population_with_fitness]\n",
    "        # sort the fitness\n",
    "        sorted_fitness_scores = [fitness for fitness, _ in sorted_population_with_fitness]\n",
    "        \"\"\"\n",
    "        indices = np.argsort(fitness_scores)\n",
    "        sorted_population = [population[i] for i in indices]\n",
    "        sorted_fitness_scores = [fitness_scores[i] for i in indices]\n",
    "        \n",
    "        return sorted_population, sorted_fitness_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 : GA Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_size = 100\n",
    "numbers_of_generations = 200\n",
    "num_parameters = 369\n",
    "mutation_rate=0.1\n",
    "tournament_size = 5\n",
    "elitism_count = 10  # Keep top 10 solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 : Optimization Process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: Best fitness 0.09109437474686773 for train, fitness 0.0867540181549066 for test\n",
      "Generation 2: Best fitness 0.07167893941008747 for train, fitness 0.07620866260799988 for test\n",
      "Generation 3: Best fitness 0.07167893941008747 for train, fitness 0.07620866260799988 for test\n",
      "Generation 4: Best fitness 0.0414831029328656 for train, fitness 0.039598248046748885 for test\n",
      "Generation 5: Best fitness 0.0327163474961755 for train, fitness 0.03447103863979561 for test\n",
      "Generation 6: Best fitness 0.0327163474961755 for train, fitness 0.03447103863979561 for test\n",
      "Generation 7: Best fitness 0.030204536563112984 for train, fitness 0.02815612563428352 for test\n",
      "Generation 8: Best fitness 0.011121992602118746 for train, fitness 0.009504218280373813 for test\n",
      "Generation 9: Best fitness 0.011121992602118746 for train, fitness 0.009504218280373813 for test\n",
      "Generation 10: Best fitness 0.011121992602118746 for train, fitness 0.009504218280373813 for test\n",
      "Generation 11: Best fitness 0.011121992602118746 for train, fitness 0.009504218280373813 for test\n",
      "Generation 12: Best fitness 0.011121992602118746 for train, fitness 0.009504218280373813 for test\n",
      "Generation 13: Best fitness 0.011121992602118746 for train, fitness 0.009504218280373813 for test\n",
      "Generation 14: Best fitness 0.011121992602118746 for train, fitness 0.009504218280373813 for test\n",
      "Generation 15: Best fitness 0.008773237037079164 for train, fitness 0.008003244209428627 for test\n",
      "Generation 16: Best fitness 0.008766086803887343 for train, fitness 0.008436889761590703 for test\n",
      "Generation 17: Best fitness 0.006746218858611689 for train, fitness 0.006064747746951063 for test\n",
      "Generation 18: Best fitness 0.005288935065374858 for train, fitness 0.005087580206742326 for test\n",
      "Generation 19: Best fitness 0.005288935065374858 for train, fitness 0.005087580206742326 for test\n",
      "Generation 20: Best fitness 0.004986411044668531 for train, fitness 0.004496095125172303 for test\n",
      "Generation 21: Best fitness 0.004986411044668531 for train, fitness 0.004496095125172303 for test\n",
      "Generation 22: Best fitness 0.004986411044668531 for train, fitness 0.004496095125172303 for test\n",
      "Generation 23: Best fitness 0.0048039466715536164 for train, fitness 0.004146384961790176 for test\n",
      "Generation 24: Best fitness 0.004165598125277467 for train, fitness 0.0045922148727565535 for test\n",
      "Generation 25: Best fitness 0.003525749928880097 for train, fitness 0.003464465032428548 for test\n",
      "Generation 26: Best fitness 0.0030065091029067366 for train, fitness 0.002846768253896259 for test\n",
      "Generation 27: Best fitness 0.0030065091029067366 for train, fitness 0.002846768253896259 for test\n",
      "Generation 28: Best fitness 0.002899301228536892 for train, fitness 0.0029639592162291895 for test\n",
      "Generation 29: Best fitness 0.002706668193881976 for train, fitness 0.002617846422167731 for test\n",
      "Generation 30: Best fitness 0.0022822251175644306 for train, fitness 0.002080621888684706 for test\n",
      "Generation 31: Best fitness 0.0022822251175644306 for train, fitness 0.002080621888684706 for test\n",
      "Generation 32: Best fitness 0.0022822251175644306 for train, fitness 0.002080621888684706 for test\n",
      "Generation 33: Best fitness 0.0022822251175644306 for train, fitness 0.002080621888684706 for test\n",
      "Generation 34: Best fitness 0.0022718781819705963 for train, fitness 0.002240738846270068 for test\n",
      "Generation 35: Best fitness 0.0022718781819705963 for train, fitness 0.002240738846270068 for test\n",
      "Generation 36: Best fitness 0.001959088407393529 for train, fitness 0.0018055547105860514 for test\n",
      "Generation 37: Best fitness 0.001959088407393529 for train, fitness 0.0018055547105860514 for test\n",
      "Generation 38: Best fitness 0.0016047153803776255 for train, fitness 0.001536582208525825 for test\n",
      "Generation 39: Best fitness 0.0016047153803776255 for train, fitness 0.001536582208525825 for test\n",
      "Generation 40: Best fitness 0.0016047153803776255 for train, fitness 0.001536582208525825 for test\n",
      "Generation 41: Best fitness 0.0014196797055933152 for train, fitness 0.0012977985737600017 for test\n",
      "Generation 42: Best fitness 0.001114511933181921 for train, fitness 0.0010274935054658968 for test\n",
      "Generation 43: Best fitness 0.001114511933181921 for train, fitness 0.0010274935054658968 for test\n",
      "Generation 44: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 45: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 46: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 47: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 48: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 49: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 50: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 51: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 52: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 53: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 54: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 55: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 56: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 57: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 58: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 59: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 60: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 61: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 62: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 63: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 64: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 65: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 66: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 67: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 68: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 69: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 70: Best fitness 0.0006369860050548632 for train, fitness 0.0007252975989908761 for test\n",
      "Generation 71: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 72: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 73: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 74: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 75: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 76: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 77: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 78: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 79: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 80: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 81: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 82: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 83: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 84: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 85: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 86: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 87: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 88: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 89: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 90: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 91: Best fitness 0.0004991667802624398 for train, fitness 0.0005680455031287898 for test\n",
      "Generation 92: Best fitness 0.000489361431029884 for train, fitness 0.0005653452642367971 for test\n",
      "Generation 93: Best fitness 0.000489361431029884 for train, fitness 0.0005653452642367971 for test\n",
      "Generation 94: Best fitness 0.000489361431029884 for train, fitness 0.0005653452642367971 for test\n",
      "Generation 95: Best fitness 0.000489361431029884 for train, fitness 0.0005653452642367971 for test\n",
      "Generation 96: Best fitness 0.000489361431029884 for train, fitness 0.0005653452642367971 for test\n",
      "Generation 97: Best fitness 0.0004515271188128593 for train, fitness 0.0004869781796150418 for test\n",
      "Generation 98: Best fitness 0.0004515271188128593 for train, fitness 0.0004869781796150418 for test\n",
      "Generation 99: Best fitness 0.0004515271188128593 for train, fitness 0.0004869781796150418 for test\n",
      "Generation 100: Best fitness 0.0004515271188128593 for train, fitness 0.0004869781796150418 for test\n",
      "Generation 101: Best fitness 0.0004515271188128593 for train, fitness 0.0004869781796150418 for test\n",
      "Generation 102: Best fitness 0.0004515271188128593 for train, fitness 0.0004869781796150418 for test\n",
      "Generation 103: Best fitness 0.0004515271188128593 for train, fitness 0.0004869781796150418 for test\n",
      "Generation 104: Best fitness 0.0004515271188128593 for train, fitness 0.0004869781796150418 for test\n",
      "Generation 105: Best fitness 0.0004515271188128593 for train, fitness 0.0004869781796150418 for test\n",
      "Generation 106: Best fitness 0.0004515271188128593 for train, fitness 0.0004869781796150418 for test\n",
      "Generation 107: Best fitness 0.0004515271188128593 for train, fitness 0.0004869781796150418 for test\n",
      "Generation 108: Best fitness 0.0003330277773380276 for train, fitness 0.00036312133770050303 for test\n",
      "Generation 109: Best fitness 0.0003330277773380276 for train, fitness 0.00036312133770050303 for test\n",
      "Generation 110: Best fitness 0.0003330277773380276 for train, fitness 0.00036312133770050303 for test\n",
      "Generation 111: Best fitness 0.0003330277773380276 for train, fitness 0.00036312133770050303 for test\n",
      "Generation 112: Best fitness 0.0003330277773380276 for train, fitness 0.00036312133770050303 for test\n",
      "Generation 113: Best fitness 0.0003330277773380276 for train, fitness 0.00036312133770050303 for test\n",
      "Generation 114: Best fitness 0.0003330277773380276 for train, fitness 0.00036312133770050303 for test\n",
      "Generation 115: Best fitness 0.0003330277773380276 for train, fitness 0.00036312133770050303 for test\n",
      "Generation 116: Best fitness 0.0003330277773380276 for train, fitness 0.00036312133770050303 for test\n",
      "Generation 117: Best fitness 0.0003330277773380276 for train, fitness 0.00036312133770050303 for test\n",
      "Generation 118: Best fitness 0.0003330277773380276 for train, fitness 0.00036312133770050303 for test\n",
      "Generation 119: Best fitness 0.0003046554416544679 for train, fitness 0.00035206552577229716 for test\n",
      "Generation 120: Best fitness 0.0003046554416544679 for train, fitness 0.00035206552577229716 for test\n",
      "Generation 121: Best fitness 0.0002603787455925049 for train, fitness 0.0002664881102940326 for test\n",
      "Generation 122: Best fitness 0.0002603787455925049 for train, fitness 0.0002664881102940326 for test\n",
      "Generation 123: Best fitness 0.0002603787455925049 for train, fitness 0.0002664881102940326 for test\n",
      "Generation 124: Best fitness 0.0002603787455925049 for train, fitness 0.0002664881102940326 for test\n",
      "Generation 125: Best fitness 0.0002603787455925049 for train, fitness 0.0002664881102940326 for test\n",
      "Generation 126: Best fitness 0.0002603787455925049 for train, fitness 0.0002664881102940326 for test\n",
      "Generation 127: Best fitness 0.0002513244251068936 for train, fitness 0.0003104307637351654 for test\n",
      "Generation 128: Best fitness 0.0002513244251068936 for train, fitness 0.0003104307637351654 for test\n",
      "Generation 129: Best fitness 0.0002513244251068936 for train, fitness 0.0003104307637351654 for test\n",
      "Generation 130: Best fitness 0.0002513244251068936 for train, fitness 0.0003104307637351654 for test\n",
      "Generation 131: Best fitness 0.0002513244251068936 for train, fitness 0.0003104307637351654 for test\n",
      "Generation 132: Best fitness 0.00023565684427717627 for train, fitness 0.0002661074655646319 for test\n",
      "Generation 133: Best fitness 0.0002121501490829015 for train, fitness 0.0002387645765231975 for test\n",
      "Generation 134: Best fitness 0.0002121501490829015 for train, fitness 0.0002387645765231975 for test\n",
      "Generation 135: Best fitness 0.0002121501490829015 for train, fitness 0.0002387645765231975 for test\n",
      "Generation 136: Best fitness 0.0002121501490829015 for train, fitness 0.0002387645765231975 for test\n",
      "Generation 137: Best fitness 0.0002121501490829015 for train, fitness 0.0002387645765231975 for test\n",
      "Generation 138: Best fitness 0.0002121501490829015 for train, fitness 0.0002387645765231975 for test\n",
      "Generation 139: Best fitness 0.00018986364126353318 for train, fitness 0.00020362989109674453 for test\n",
      "Generation 140: Best fitness 0.00018986364126353318 for train, fitness 0.00020362989109674453 for test\n",
      "Generation 141: Best fitness 0.00018986364126353318 for train, fitness 0.00020362989109674453 for test\n",
      "Generation 142: Best fitness 0.00018986364126353318 for train, fitness 0.00020362989109674453 for test\n",
      "Generation 143: Best fitness 0.00018873519270946963 for train, fitness 0.0002037277046678292 for test\n",
      "Generation 144: Best fitness 0.00017082881448297923 for train, fitness 0.00018745485414821975 for test\n",
      "Generation 145: Best fitness 0.00017082881448297923 for train, fitness 0.00018745485414821975 for test\n",
      "Generation 146: Best fitness 0.00017082881448297923 for train, fitness 0.00018745485414821975 for test\n",
      "Generation 147: Best fitness 0.00017082881448297923 for train, fitness 0.00018745485414821975 for test\n",
      "Generation 148: Best fitness 0.00015610308951640117 for train, fitness 0.00017062962322286562 for test\n",
      "Generation 149: Best fitness 0.00015610308951640117 for train, fitness 0.00017062962322286562 for test\n",
      "Generation 150: Best fitness 0.00015610308951640117 for train, fitness 0.00017062962322286562 for test\n",
      "Generation 151: Best fitness 0.00013991563280624288 for train, fitness 0.0001530266428888364 for test\n",
      "Generation 152: Best fitness 0.00013902632505568622 for train, fitness 0.00017397199593832484 for test\n",
      "Generation 153: Best fitness 0.00013902632505568622 for train, fitness 0.00017397199593832484 for test\n",
      "Generation 154: Best fitness 0.00013902632505568622 for train, fitness 0.00017397199593832484 for test\n",
      "Generation 155: Best fitness 0.00013902632505568622 for train, fitness 0.00017397199593832484 for test\n",
      "Generation 156: Best fitness 0.00013902632505568622 for train, fitness 0.00017397199593832484 for test\n",
      "Generation 157: Best fitness 0.00013902632505568622 for train, fitness 0.00017397199593832484 for test\n",
      "Generation 158: Best fitness 0.00013902632505568622 for train, fitness 0.00017397199593832484 for test\n",
      "Generation 159: Best fitness 0.00013902632505568622 for train, fitness 0.00017397199593832484 for test\n",
      "Generation 160: Best fitness 0.00013902632505568622 for train, fitness 0.00017397199593832484 for test\n",
      "Generation 161: Best fitness 0.00013902632505568622 for train, fitness 0.00017397199593832484 for test\n",
      "Generation 162: Best fitness 0.00013902632505568622 for train, fitness 0.00017397199593832484 for test\n",
      "Generation 163: Best fitness 0.00013902632505568622 for train, fitness 0.00017397199593832484 for test\n",
      "Generation 164: Best fitness 0.00011269334437783709 for train, fitness 0.00012439298725227486 for test\n",
      "Generation 165: Best fitness 0.00011269334437783709 for train, fitness 0.00012439298725227486 for test\n",
      "Generation 166: Best fitness 0.00011269334437783709 for train, fitness 0.00012439298725227486 for test\n",
      "Generation 167: Best fitness 0.00011269334437783709 for train, fitness 0.00012439298725227486 for test\n",
      "Generation 168: Best fitness 0.00011269334437783709 for train, fitness 0.00012439298725227486 for test\n",
      "Generation 169: Best fitness 0.00011269334437783709 for train, fitness 0.00012439298725227486 for test\n",
      "Generation 170: Best fitness 0.00011269334437783709 for train, fitness 0.00012439298725227486 for test\n",
      "Generation 171: Best fitness 0.00010050971791302626 for train, fitness 0.00011045556671339699 for test\n",
      "Generation 172: Best fitness 0.00010050971791302626 for train, fitness 0.00011045556671339699 for test\n",
      "Generation 173: Best fitness 0.00010050971791302626 for train, fitness 0.00011045556671339699 for test\n",
      "Generation 174: Best fitness 0.00010050971791302626 for train, fitness 0.00011045556671339699 for test\n",
      "Generation 175: Best fitness 0.00010050971791302626 for train, fitness 0.00011045556671339699 for test\n",
      "Generation 176: Best fitness 0.00010050971791302626 for train, fitness 0.00011045556671339699 for test\n",
      "Generation 177: Best fitness 0.00010050971791302626 for train, fitness 0.00011045556671339699 for test\n",
      "Generation 178: Best fitness 0.00010050971791302626 for train, fitness 0.00011045556671339699 for test\n",
      "Generation 179: Best fitness 0.00010050971791302626 for train, fitness 0.00011045556671339699 for test\n",
      "Generation 180: Best fitness 9.156916579061507e-05 for train, fitness 0.00010734596467660833 for test\n",
      "Generation 181: Best fitness 9.156916579061507e-05 for train, fitness 0.00010734596467660833 for test\n",
      "Generation 182: Best fitness 9.156916579061507e-05 for train, fitness 0.00010734596467660833 for test\n",
      "Generation 183: Best fitness 9.156916579061507e-05 for train, fitness 0.00010734596467660833 for test\n",
      "Generation 184: Best fitness 9.156916579061507e-05 for train, fitness 0.00010734596467660833 for test\n",
      "Generation 185: Best fitness 9.156916579061507e-05 for train, fitness 0.00010734596467660833 for test\n",
      "Generation 186: Best fitness 9.156916579061507e-05 for train, fitness 0.00010734596467660833 for test\n",
      "Generation 187: Best fitness 7.876168338830806e-05 for train, fitness 8.688007757305529e-05 for test\n",
      "Generation 188: Best fitness 7.876168338830806e-05 for train, fitness 8.688007757305529e-05 for test\n",
      "Generation 189: Best fitness 7.052522350211156e-05 for train, fitness 6.298131259526403e-05 for test\n",
      "Generation 190: Best fitness 7.052522350211156e-05 for train, fitness 6.298131259526403e-05 for test\n",
      "Generation 191: Best fitness 7.052522350211156e-05 for train, fitness 6.298131259526403e-05 for test\n",
      "Generation 192: Best fitness 7.052522350211156e-05 for train, fitness 6.298131259526403e-05 for test\n",
      "Generation 193: Best fitness 7.052522350211156e-05 for train, fitness 6.298131259526403e-05 for test\n",
      "Generation 194: Best fitness 7.052522350211156e-05 for train, fitness 6.298131259526403e-05 for test\n",
      "Generation 195: Best fitness 7.052522350211156e-05 for train, fitness 6.298131259526403e-05 for test\n",
      "Generation 196: Best fitness 7.052522350211156e-05 for train, fitness 6.298131259526403e-05 for test\n",
      "Generation 197: Best fitness 7.052522350211156e-05 for train, fitness 6.298131259526403e-05 for test\n",
      "Generation 198: Best fitness 7.052522350211156e-05 for train, fitness 6.298131259526403e-05 for test\n",
      "Generation 199: Best fitness 6.396708207198313e-05 for train, fitness 6.777065798687889e-05 for test\n",
      "Generation 200: Best fitness 6.396708207198313e-05 for train, fitness 6.777065798687889e-05 for test\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = create_model()\n",
    "GA = GeneticAlgorithm(model,X_train,y_train)\n",
    "\n",
    "\n",
    "best_mse_per_generation_train = []\n",
    "best_mse_per_generation_test = []\n",
    "population=GA.initialize_population(population_size,num_parameters)\n",
    "for generation in range(numbers_of_generations):\n",
    "    # Step 1: Evaluate fitness for each individual for the elitism function\n",
    "    fitness_values = [GA.fitness(indivual,X_train,y_train) for indivual in population]\n",
    "\n",
    "    # Sort individuals\n",
    "    sorted_population,sorted_fitness = GA.sort_population(population,fitness_values)\n",
    "\n",
    "    # Step 3 : Select the 10 best individuals in the population\n",
    "    elites = sorted_population[:elitism_count]\n",
    "    # Step 2: Perform tournament selection \n",
    "    # We apply it only on the 90th others individuals\n",
    "    tournament_winners = GA.Tournament_Selection(sorted_population[elitism_count:], tournament_size, population_size - elitism_count,sorted_fitness[elitism_count:])\n",
    "    # Step 3: Apply crossover and mutation\n",
    "    crossover_population = GA.apply_crossover(tournament_winners, len(tournament_winners))\n",
    "    mutated_population = GA.apply_mutation(crossover_population, len(crossover_population), mutation_rate)\n",
    "    \n",
    "    # Step 4: Create a new generation with 10th whoose with elitism and the others\n",
    "    population = elites+mutated_population\n",
    "\n",
    "    # We compute the fitness for test using the best population we found for train\n",
    "    fitness_test = GA.fitness(elites[0],X_test,y_test)\n",
    "\n",
    "    print(f\"Generation {generation + 1}: Best fitness {sorted_fitness[0]} for train, fitness {fitness_test} for test\")\n",
    "\n",
    "    best_mse_per_generation_train.append(sorted_fitness[0])\n",
    "    best_mse_per_generation_test.append(fitness_test)\n",
    "    \n",
    "    GA.update_model(sorted_population[0]) # Because it's ask to update it at each generation even if it will be change in the next generation\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Affichage de la première courbe (par exemple, MSE)\n",
    "plt.plot(range(1, numbers_of_generations + 1), best_mse_per_generation_train, label='Train')\n",
    "\n",
    "# Affichage de la deuxième courbe (par exemple, une autre métrique, ici \"accuracy\")\n",
    "plt.plot(range(1, numbers_of_generations + 1), best_mse_per_generation_test, label='Test')\n",
    "\n",
    "# Ajouter des légendes pour différencier les courbes\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Evolution of MSE and Accuracy over Generations\")\n",
    "\n",
    "# Afficher la légende pour identifier chaque courbe\n",
    "plt.legend()\n",
    "\n",
    "# Affichage du graphique\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model perform really good on the train and also on the test. Thus there is no overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 : Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try to use the traditional methods and compare the results\n",
    "\n",
    "To do that we create a new function callback that will allow to print the mse for each epoch of the train and the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class TrainTestPerformanceCallback(Callback):\n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        super().__init__()\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Calculer la performance sur les données d'entraînement\n",
    "        train_loss,_ = self.model.evaluate(self.X_train, self.y_train, verbose=0)\n",
    "        \n",
    "        # Calculer la performance sur les données de test\n",
    "        test_loss,_ = self.model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "        \n",
    "        # Afficher les résultats\n",
    "        print(f\"Epoch {epoch + 1}: mse = {train_loss} for train, mse = {test_loss} for test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: mse = 0.23897747695446014 for train, mse = 0.26697468757629395 for test\n",
      "Epoch 2: mse = 0.2363714724779129 for train, mse = 0.2623763978481293 for test\n",
      "Epoch 3: mse = 0.23393180966377258 for train, mse = 0.2581613063812256 for test\n",
      "Epoch 4: mse = 0.23156261444091797 for train, mse = 0.2539840340614319 for test\n",
      "Epoch 5: mse = 0.22929927706718445 for train, mse = 0.25003498792648315 for test\n",
      "Epoch 6: mse = 0.22710900008678436 for train, mse = 0.24622742831707 for test\n",
      "Epoch 7: mse = 0.224982351064682 for train, mse = 0.24256446957588196 for test\n",
      "Epoch 8: mse = 0.22291946411132812 for train, mse = 0.2391178160905838 for test\n",
      "Epoch 9: mse = 0.22090300917625427 for train, mse = 0.23567180335521698 for test\n",
      "Epoch 10: mse = 0.21897146105766296 for train, mse = 0.2326829433441162 for test\n",
      "Epoch 11: mse = 0.21705901622772217 for train, mse = 0.22977972030639648 for test\n",
      "Epoch 12: mse = 0.21515598893165588 for train, mse = 0.22675995528697968 for test\n",
      "Epoch 13: mse = 0.21327140927314758 for train, mse = 0.22392591834068298 for test\n",
      "Epoch 14: mse = 0.21138134598731995 for train, mse = 0.22116205096244812 for test\n",
      "Epoch 15: mse = 0.20947770774364471 for train, mse = 0.21852020919322968 for test\n",
      "Epoch 16: mse = 0.20755788683891296 for train, mse = 0.21586544811725616 for test\n",
      "Epoch 17: mse = 0.20561210811138153 for train, mse = 0.2132982760667801 for test\n",
      "Epoch 18: mse = 0.20364800095558167 for train, mse = 0.2107151448726654 for test\n",
      "Epoch 19: mse = 0.2016507089138031 for train, mse = 0.20823058485984802 for test\n",
      "Epoch 20: mse = 0.1996270716190338 for train, mse = 0.20574049651622772 for test\n",
      "Epoch 21: mse = 0.19757167994976044 for train, mse = 0.20328769087791443 for test\n",
      "Epoch 22: mse = 0.1954900324344635 for train, mse = 0.20086880028247833 for test\n",
      "Epoch 23: mse = 0.19341260194778442 for train, mse = 0.19832658767700195 for test\n",
      "Epoch 24: mse = 0.19128403067588806 for train, mse = 0.19590982794761658 for test\n",
      "Epoch 25: mse = 0.1891271024942398 for train, mse = 0.19349756836891174 for test\n",
      "Epoch 26: mse = 0.1869678497314453 for train, mse = 0.19099731743335724 for test\n",
      "Epoch 27: mse = 0.18476134538650513 for train, mse = 0.1885909140110016 for test\n",
      "Epoch 28: mse = 0.18252220749855042 for train, mse = 0.18619360029697418 for test\n",
      "Epoch 29: mse = 0.18029555678367615 for train, mse = 0.18369396030902863 for test\n",
      "Epoch 30: mse = 0.17802444100379944 for train, mse = 0.18128064274787903 for test\n",
      "Epoch 31: mse = 0.175717294216156 for train, mse = 0.1789507269859314 for test\n",
      "Epoch 32: mse = 0.17341285943984985 for train, mse = 0.17650948464870453 for test\n",
      "Epoch 33: mse = 0.1710910201072693 for train, mse = 0.17404574155807495 for test\n",
      "Epoch 34: mse = 0.1687328815460205 for train, mse = 0.17165513336658478 for test\n",
      "Epoch 35: mse = 0.16637983918190002 for train, mse = 0.16916319727897644 for test\n",
      "Epoch 36: mse = 0.16399416327476501 for train, mse = 0.1667478084564209 for test\n",
      "Epoch 37: mse = 0.16160087287425995 for train, mse = 0.16429129242897034 for test\n",
      "Epoch 38: mse = 0.15920977294445038 for train, mse = 0.16175271570682526 for test\n",
      "Epoch 39: mse = 0.15678927302360535 for train, mse = 0.15929174423217773 for test\n",
      "Epoch 40: mse = 0.15436521172523499 for train, mse = 0.15677422285079956 for test\n",
      "Epoch 41: mse = 0.15193109214305878 for train, mse = 0.1542430818080902 for test\n",
      "Epoch 42: mse = 0.14949475228786469 for train, mse = 0.15165390074253082 for test\n",
      "Epoch 43: mse = 0.14704559743404388 for train, mse = 0.14907869696617126 for test\n",
      "Epoch 44: mse = 0.14458172023296356 for train, mse = 0.14651578664779663 for test\n",
      "Epoch 45: mse = 0.14210286736488342 for train, mse = 0.14398808777332306 for test\n",
      "Epoch 46: mse = 0.1396256983280182 for train, mse = 0.1414192169904709 for test\n",
      "Epoch 47: mse = 0.1371472328901291 for train, mse = 0.13881203532218933 for test\n",
      "Epoch 48: mse = 0.1346559226512909 for train, mse = 0.13624821603298187 for test\n",
      "Epoch 49: mse = 0.13216087222099304 for train, mse = 0.13368555903434753 for test\n",
      "Epoch 50: mse = 0.12966863811016083 for train, mse = 0.13108766078948975 for test\n",
      "Epoch 51: mse = 0.1271713674068451 for train, mse = 0.12850165367126465 for test\n",
      "Epoch 52: mse = 0.12468104064464569 for train, mse = 0.12587332725524902 for test\n",
      "Epoch 53: mse = 0.12220004945993423 for train, mse = 0.12322066724300385 for test\n",
      "Epoch 54: mse = 0.11971424520015717 for train, mse = 0.12060844898223877 for test\n",
      "Epoch 55: mse = 0.11722350120544434 for train, mse = 0.11804284900426865 for test\n",
      "Epoch 56: mse = 0.11473806947469711 for train, mse = 0.11545862257480621 for test\n",
      "Epoch 57: mse = 0.11225328594446182 for train, mse = 0.11288990080356598 for test\n",
      "Epoch 58: mse = 0.10977685451507568 for train, mse = 0.11032257974147797 for test\n",
      "Epoch 59: mse = 0.10731349140405655 for train, mse = 0.10774632543325424 for test\n",
      "Epoch 60: mse = 0.10485009104013443 for train, mse = 0.10523667186498642 for test\n",
      "Epoch 61: mse = 0.10239958763122559 for train, mse = 0.10273224115371704 for test\n",
      "Epoch 62: mse = 0.09997230768203735 for train, mse = 0.1001928448677063 for test\n",
      "Epoch 63: mse = 0.09755424410104752 for train, mse = 0.09769944846630096 for test\n",
      "Epoch 64: mse = 0.09515596181154251 for train, mse = 0.0952099859714508 for test\n",
      "Epoch 65: mse = 0.092771515250206 for train, mse = 0.09275762736797333 for test\n",
      "Epoch 66: mse = 0.0904063954949379 for train, mse = 0.09034381806850433 for test\n",
      "Epoch 67: mse = 0.08806861937046051 for train, mse = 0.08793185651302338 for test\n",
      "Epoch 68: mse = 0.08575432002544403 for train, mse = 0.08554894477128983 for test\n",
      "Epoch 69: mse = 0.08346789330244064 for train, mse = 0.08318942785263062 for test\n",
      "Epoch 70: mse = 0.08120976388454437 for train, mse = 0.08086268603801727 for test\n",
      "Epoch 71: mse = 0.07898460328578949 for train, mse = 0.07856257259845734 for test\n",
      "Epoch 72: mse = 0.07679320871829987 for train, mse = 0.07629980891942978 for test\n",
      "Epoch 73: mse = 0.07463761419057846 for train, mse = 0.07407601922750473 for test\n",
      "Epoch 74: mse = 0.07251686602830887 for train, mse = 0.07190708070993423 for test\n",
      "Epoch 75: mse = 0.07043571770191193 for train, mse = 0.06977764517068863 for test\n",
      "Epoch 76: mse = 0.06839878857135773 for train, mse = 0.06767942011356354 for test\n",
      "Epoch 77: mse = 0.06640280783176422 for train, mse = 0.06562943756580353 for test\n",
      "Epoch 78: mse = 0.06445164978504181 for train, mse = 0.06362469494342804 for test\n",
      "Epoch 79: mse = 0.06254872679710388 for train, mse = 0.061669934540987015 for test\n",
      "Epoch 80: mse = 0.060688428580760956 for train, mse = 0.05977505445480347 for test\n",
      "Epoch 81: mse = 0.05887581780552864 for train, mse = 0.05792872980237007 for test\n",
      "Epoch 82: mse = 0.057110559195280075 for train, mse = 0.05614307522773743 for test\n",
      "Epoch 83: mse = 0.05539242550730705 for train, mse = 0.05442674830555916 for test\n",
      "Epoch 84: mse = 0.05372877046465874 for train, mse = 0.0527355782687664 for test\n",
      "Epoch 85: mse = 0.05211149528622627 for train, mse = 0.051118768751621246 for test\n",
      "Epoch 86: mse = 0.050543926656246185 for train, mse = 0.049545202404260635 for test\n",
      "Epoch 87: mse = 0.04902845621109009 for train, mse = 0.04801452159881592 for test\n",
      "Epoch 88: mse = 0.04755859449505806 for train, mse = 0.04655040055513382 for test\n",
      "Epoch 89: mse = 0.04613813757896423 for train, mse = 0.04512786120176315 for test\n",
      "Epoch 90: mse = 0.044766657054424286 for train, mse = 0.04375261440873146 for test\n",
      "Epoch 91: mse = 0.0434393584728241 for train, mse = 0.04243947193026543 for test\n",
      "Epoch 92: mse = 0.04215977340936661 for train, mse = 0.04116862267255783 for test\n",
      "Epoch 93: mse = 0.0409269705414772 for train, mse = 0.03993891924619675 for test\n",
      "Epoch 94: mse = 0.039735402911901474 for train, mse = 0.03876455873250961 for test\n",
      "Epoch 95: mse = 0.038587797433137894 for train, mse = 0.03763670474290848 for test\n",
      "Epoch 96: mse = 0.03748125955462456 for train, mse = 0.03654817119240761 for test\n",
      "Epoch 97: mse = 0.036416858434677124 for train, mse = 0.03549984097480774 for test\n",
      "Epoch 98: mse = 0.03539111092686653 for train, mse = 0.03450033813714981 for test\n",
      "Epoch 99: mse = 0.03440357744693756 for train, mse = 0.03353922441601753 for test\n",
      "Epoch 100: mse = 0.03345513343811035 for train, mse = 0.03261304646730423 for test\n",
      "Epoch 101: mse = 0.032541532069444656 for train, mse = 0.03172548487782478 for test\n",
      "Epoch 102: mse = 0.0316627211868763 for train, mse = 0.030870545655488968 for test\n",
      "Epoch 103: mse = 0.03081654943525791 for train, mse = 0.030048394575715065 for test\n",
      "Epoch 104: mse = 0.030003711581230164 for train, mse = 0.029262131080031395 for test\n",
      "Epoch 105: mse = 0.029220426455140114 for train, mse = 0.028508543968200684 for test\n",
      "Epoch 106: mse = 0.028468411415815353 for train, mse = 0.02778218314051628 for test\n",
      "Epoch 107: mse = 0.027742907404899597 for train, mse = 0.027086859568953514 for test\n",
      "Epoch 108: mse = 0.027046605944633484 for train, mse = 0.026413362473249435 for test\n",
      "Epoch 109: mse = 0.026375699788331985 for train, mse = 0.02576967142522335 for test\n",
      "Epoch 110: mse = 0.025730658322572708 for train, mse = 0.025150638073682785 for test\n",
      "Epoch 111: mse = 0.025109106674790382 for train, mse = 0.024556748569011688 for test\n",
      "Epoch 112: mse = 0.02451005019247532 for train, mse = 0.02398546412587166 for test\n",
      "Epoch 113: mse = 0.023932700976729393 for train, mse = 0.023434903472661972 for test\n",
      "Epoch 114: mse = 0.023377487435936928 for train, mse = 0.022903021425008774 for test\n",
      "Epoch 115: mse = 0.022843601182103157 for train, mse = 0.02238958142697811 for test\n",
      "Epoch 116: mse = 0.022326814010739326 for train, mse = 0.0218959953635931 for test\n",
      "Epoch 117: mse = 0.021829385310411453 for train, mse = 0.02142031490802765 for test\n",
      "Epoch 118: mse = 0.021350720897316933 for train, mse = 0.020957540720701218 for test\n",
      "Epoch 119: mse = 0.020888689905405045 for train, mse = 0.020514529198408127 for test\n",
      "Epoch 120: mse = 0.020441917702555656 for train, mse = 0.02008853480219841 for test\n",
      "Epoch 121: mse = 0.020011719316244125 for train, mse = 0.019674813374876976 for test\n",
      "Epoch 122: mse = 0.019593972712755203 for train, mse = 0.019275357946753502 for test\n",
      "Epoch 123: mse = 0.019191745668649673 for train, mse = 0.018887966871261597 for test\n",
      "Epoch 124: mse = 0.018802855163812637 for train, mse = 0.01851373165845871 for test\n",
      "Epoch 125: mse = 0.018425418063998222 for train, mse = 0.0181546863168478 for test\n",
      "Epoch 126: mse = 0.018061622977256775 for train, mse = 0.017807161435484886 for test\n",
      "Epoch 127: mse = 0.01770859956741333 for train, mse = 0.01747102476656437 for test\n",
      "Epoch 128: mse = 0.017366895452141762 for train, mse = 0.01714419201016426 for test\n",
      "Epoch 129: mse = 0.017036395147442818 for train, mse = 0.01682594045996666 for test\n",
      "Epoch 130: mse = 0.01671619340777397 for train, mse = 0.01651807315647602 for test\n",
      "Epoch 131: mse = 0.016405269503593445 for train, mse = 0.01622154377400875 for test\n",
      "Epoch 132: mse = 0.01610400900244713 for train, mse = 0.015933390706777573 for test\n",
      "Epoch 133: mse = 0.01581154763698578 for train, mse = 0.015654880553483963 for test\n",
      "Epoch 134: mse = 0.015528055839240551 for train, mse = 0.015382589772343636 for test\n",
      "Epoch 135: mse = 0.015253499150276184 for train, mse = 0.015118387527763844 for test\n",
      "Epoch 136: mse = 0.014986671507358551 for train, mse = 0.014863011427223682 for test\n",
      "Epoch 137: mse = 0.014727932401001453 for train, mse = 0.01461433432996273 for test\n",
      "Epoch 138: mse = 0.014476235024631023 for train, mse = 0.01437078695744276 for test\n",
      "Epoch 139: mse = 0.014231428503990173 for train, mse = 0.014136250130832195 for test\n",
      "Epoch 140: mse = 0.013993005268275738 for train, mse = 0.013907527551054955 for test\n",
      "Epoch 141: mse = 0.013761145994067192 for train, mse = 0.013685941696166992 for test\n",
      "Epoch 142: mse = 0.013536522164940834 for train, mse = 0.013467805460095406 for test\n",
      "Epoch 143: mse = 0.013317671604454517 for train, mse = 0.013256159611046314 for test\n",
      "Epoch 144: mse = 0.01310434378683567 for train, mse = 0.013049778528511524 for test\n",
      "Epoch 145: mse = 0.012896831147372723 for train, mse = 0.012848496437072754 for test\n",
      "Epoch 146: mse = 0.012694534845650196 for train, mse = 0.01265346072614193 for test\n",
      "Epoch 147: mse = 0.012497340328991413 for train, mse = 0.012463360093533993 for test\n",
      "Epoch 148: mse = 0.012305041775107384 for train, mse = 0.012277823872864246 for test\n",
      "Epoch 149: mse = 0.0121174156665802 for train, mse = 0.012097664177417755 for test\n",
      "Epoch 150: mse = 0.011934813112020493 for train, mse = 0.0119211096316576 for test\n",
      "Epoch 151: mse = 0.011756757274270058 for train, mse = 0.011748500168323517 for test\n",
      "Epoch 152: mse = 0.011583435349166393 for train, mse = 0.011580674909055233 for test\n",
      "Epoch 153: mse = 0.011413770727813244 for train, mse = 0.011416668072342873 for test\n",
      "Epoch 154: mse = 0.01124837901443243 for train, mse = 0.011257035657763481 for test\n",
      "Epoch 155: mse = 0.011087106540799141 for train, mse = 0.011098929680883884 for test\n",
      "Epoch 156: mse = 0.010929417796432972 for train, mse = 0.010946080088615417 for test\n",
      "Epoch 157: mse = 0.010775660164654255 for train, mse = 0.010795926675200462 for test\n",
      "Epoch 158: mse = 0.010625245049595833 for train, mse = 0.010649416595697403 for test\n",
      "Epoch 159: mse = 0.010477953590452671 for train, mse = 0.010506397113204002 for test\n",
      "Epoch 160: mse = 0.010334154590964317 for train, mse = 0.010366586036980152 for test\n",
      "Epoch 161: mse = 0.010193532332777977 for train, mse = 0.010230756364762783 for test\n",
      "Epoch 162: mse = 0.010056253522634506 for train, mse = 0.010097144171595573 for test\n",
      "Epoch 163: mse = 0.009921615943312645 for train, mse = 0.009966225363314152 for test\n",
      "Epoch 164: mse = 0.009790383279323578 for train, mse = 0.009838123805820942 for test\n",
      "Epoch 165: mse = 0.009661910124123096 for train, mse = 0.009713039733469486 for test\n",
      "Epoch 166: mse = 0.009536152705550194 for train, mse = 0.009590372443199158 for test\n",
      "Epoch 167: mse = 0.009413080289959908 for train, mse = 0.0094705605879426 for test\n",
      "Epoch 168: mse = 0.00929257646203041 for train, mse = 0.009352508001029491 for test\n",
      "Epoch 169: mse = 0.009174677543342113 for train, mse = 0.00923733040690422 for test\n",
      "Epoch 170: mse = 0.009059223346412182 for train, mse = 0.009123626165091991 for test\n",
      "Epoch 171: mse = 0.008945917710661888 for train, mse = 0.00901290588080883 for test\n",
      "Epoch 172: mse = 0.008834888227283955 for train, mse = 0.008904197253286839 for test\n",
      "Epoch 173: mse = 0.008726130239665508 for train, mse = 0.008797815069556236 for test\n",
      "Epoch 174: mse = 0.008619755506515503 for train, mse = 0.008692968636751175 for test\n",
      "Epoch 175: mse = 0.008515214547514915 for train, mse = 0.008591245859861374 for test\n",
      "Epoch 176: mse = 0.00841278675943613 for train, mse = 0.00849086046218872 for test\n",
      "Epoch 177: mse = 0.008312476798892021 for train, mse = 0.008392155170440674 for test\n",
      "Epoch 178: mse = 0.008213864639401436 for train, mse = 0.00829540565609932 for test\n",
      "Epoch 179: mse = 0.008117425255477428 for train, mse = 0.008200051262974739 for test\n",
      "Epoch 180: mse = 0.008022675290703773 for train, mse = 0.00810714066028595 for test\n",
      "Epoch 181: mse = 0.007929851301014423 for train, mse = 0.008016101084649563 for test\n",
      "Epoch 182: mse = 0.00783860869705677 for train, mse = 0.00792626477777958 for test\n",
      "Epoch 183: mse = 0.0077491565607488155 for train, mse = 0.007838514633476734 for test\n",
      "Epoch 184: mse = 0.007661300245672464 for train, mse = 0.007751907221972942 for test\n",
      "Epoch 185: mse = 0.007575047202408314 for train, mse = 0.007667300291359425 for test\n",
      "Epoch 186: mse = 0.007490449585020542 for train, mse = 0.007583794184029102 for test\n",
      "Epoch 187: mse = 0.007407208438962698 for train, mse = 0.007501880172640085 for test\n",
      "Epoch 188: mse = 0.007325347512960434 for train, mse = 0.007421274669468403 for test\n",
      "Epoch 189: mse = 0.007245246320962906 for train, mse = 0.0073423185385763645 for test\n",
      "Epoch 190: mse = 0.0071664354763925076 for train, mse = 0.007264763116836548 for test\n",
      "Epoch 191: mse = 0.0070890141651034355 for train, mse = 0.0071886577643454075 for test\n",
      "Epoch 192: mse = 0.007013045251369476 for train, mse = 0.007113506551831961 for test\n",
      "Epoch 193: mse = 0.00693841278553009 for train, mse = 0.007039350923150778 for test\n",
      "Epoch 194: mse = 0.006864949129521847 for train, mse = 0.006966692861169577 for test\n",
      "Epoch 195: mse = 0.0067928568460047245 for train, mse = 0.006894973572343588 for test\n",
      "Epoch 196: mse = 0.0067217848263680935 for train, mse = 0.006824974901974201 for test\n",
      "Epoch 197: mse = 0.006651964969933033 for train, mse = 0.006755828857421875 for test\n",
      "Epoch 198: mse = 0.006583305541425943 for train, mse = 0.006688079796731472 for test\n",
      "Epoch 199: mse = 0.006515826098620892 for train, mse = 0.006621410604566336 for test\n",
      "Epoch 200: mse = 0.006449377629905939 for train, mse = 0.006555925123393536 for test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ad757f28a0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = create_model()\n",
    "performance_callback = TrainTestPerformanceCallback(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Entraîner le modèle avec la callback\n",
    "model2.fit(X_train, y_train, epochs=200, validation_split=0.2, verbose=0, callbacks=[performance_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the traditionnal methods is 2 times faster than the GA methods, \n",
    "for 200 generations :\n",
    "Traditional=> 53 seconds\n",
    "GA=> 1 minute and  40seconds\n",
    "\n",
    "the accuracy of the GA algorithms is 100 times better, \n",
    "for 200 generations : \n",
    "Traditional=> 6.5e-03\n",
    "GA=> 6.5e-05\n",
    "\n",
    "Thus the genetic algorithm is a better options for this problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
